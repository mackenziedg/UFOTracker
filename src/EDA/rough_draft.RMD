---
title: "Predicting UFO Sightings"
author: "Shane Caldwell, Fanny Chow, Mackenzie Gray, Noah Johnson"
date: "11/30/2017"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
linkcolor: blue
urlcolor: blue
fontsize: 11pt 
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(data.table)
library(tidyr)
library(leaps)
library(olsrr)
```

# Introduction

Despite the U.S. government's explicit expression of disinterest in U.F.O. research, U.F.O. Sightings have sparked interest in Americans from coast-to-coast. Avid U.F.O. enthusiasists have collected data on U.F.O. sightings throughout the years at the National U.F.O. Reporting Center. Our data analysis explores what factors or demographics make an American more likely to report a U.F.O. sighting? And what common characteristics do U.F.O. sighters share?

Our group set out to create a model to predict UFO sightings by a variety of cofactors. This was a case where the project and the analysis were created by the data at hand rather than a starting from a question.

The [National UFO Reporting Center](http://www.nuforc.org/webreports.html), established in 1974, has put together a database for citizens to submit reports of unidentified flying object sightings. One can submit reports online, but they also have a hotline that can be called at any time, and the information from the report will be placed into the database. Regardless of the medium the report is received in, it will be filtered to confirm it isn't a "hoax".

The database is moderately sized, with over 80,000 observations of reports. Reports include a timestamp, a description of the aircraft, the state and city the report came from, and the duration of the report. 

Our interest was in finding covariates to help us predict the number of reports for a specific state during a specific year. This would involve cleaning the web-scraped UFO dataset and mutating it into something closer to what we needed. It also involved carefully choosing cofactors - not a lot of literature on UFOs sightings and their trends to lean on!

# The Data

For independent variables, we ended up choosing number of breweries in a state per year, the number of alien movies released per year, per capita gdp per year, state population, portion of the population with internet access, and number of US Air Force bases in a state. Below, we argue for the "theoretical" inclusion of each variable and explain both why we chose it, where we found the data, and how we went about cleaning it. 

## Breweries Per State Per Year

Alcohol inhibits our perceptions. If you've been out drinking, perhaps you're more likely to think you saw a UFO instead of stopping at a more reasonable explanation. If there are more breweries in your state, it's more likely you'll be drinking. 

## Alien Movies Per Year

How does culture affect people's perceptions of the "supernatural"? When trailers for alien movies have been playing on TV particularly frequently for a given year, are you more likely to have aliens playing in the back of your mind when you see something in the sky you can't explain? Does watching sci-fi movies make it more likely that you turn to an organization like the UFO Reporting Center when something bright is in the sky?

Scraping for the number of alien movies released a year would be difficult, but luckily [wikipedia](https://en.wikipedia.org/wiki/List_of_films_featuring_extraterrestrials) has a list already compiled. While there is no way the list could be complete, it features movies from years as far back as 1902. The wikipedia table was scraped and turned into a csv file and a simple python script was written to bin the frequency of movies by year to create the final dataset.

## Military Bases per State

A common explanation for unidentified flying objects is of a more terrestrial, but no less secretive, origin---top secret military aircraft.
Perhaps the most famous Air Force base in the world, Area 51 in Nevada, is the rumored location of alien remains or technology.
Or mabye it's just classified experimental aviation testing.
Either way, to what extent can we explain U.F.O. sightings by their distance to a military base?

A number of immediate problems with obtaining this data spring to mind.
For one, there are almost certainly research outposts that are unknown to the general public, so these by definition will be left out of any otherwise complete list.
Additionally, having to sum over the distance to every one of the 58 Air Force bases in the US for each of the 80,000+ sightings, and then averaging those by state may not provide that much more information than simply how many bases there are per state.
From this line of thinking, we decided that the number of bases per state was a reasonable substitute for a more complicated per-sighting calculation.

The list of bases and their locations was obtained from the aptly named [list of United States Air Force installations](https://en.wikipedia.org/wiki/List_of_United_States_Air_Force_installations#United_States) on Wikipedia.


## Per Capita GDP

Is "seeing things in the sky" a poor man's game? Does the wealth of a particular state influence how they spend their time? Perhaps those with less money have nothing to do but sit outside and look into the sky?

## Portion of population with internet access by year

To even know about the UFO reporting center's existence, you need internet access. If you see a UFO and don't have access to the internet, it's unlikely you'd ever find the hotline to call to report it. It would just end up as a story you told your friends rather than a report in the UFO database. 

# EDA 

## Breweries Per State  
```{r}
beer.path <- "../../data/raw/brew_count_by_state_1984_2017.csv"
sightings.path <- "../../data/raw/ufo_sightings.csv"

beer.raw <- fread(file.path(beer.path), header=TRUE, na.strings=c("*", ""))
sightings.raw <- fread(file.path(sightings.path), header = TRUE, na.strings = c("", "Unknown", "--"))
```


```{r}
# clean up junk at bottom file
beer.db <- beer.raw %>% 
  filter(!is.na(STATE)) %>%
  filter(STATE != "Total") %>% 
  filter(STATE != "Other") %>% 
  filter(STATE != "* No reportable data") %>% 
  filter(STATE != "Â«This list will be updated quarterly.")
```


```{r}
# create proper data types
beer.db$STATE <- as.factor(beer.db$STATE)

# wide to long format
olddata_wide <- beer.db
keycol <- "year"
valuecol <- "breweries"
gathercols <- as.character(seq(1984, 2017))

beer.df <- gather_(olddata_wide, keycol, valuecol, gathercols)

```


At a high-level glance, the general trend is increase number of breweries through the years for each s tate. Note that the number of breweries in 2005 will be contingent on the number of breweries in 2004, and there will be autocorrolation through years.   
```{r}
beer.df$breweries <- as.numeric(beer.df$breweries)
```

```{r}
(ggplot() +
  geom_point(data=beer.df, aes(year, breweries, color=STATE)))
```


Let's take a look at the number of breweries in each state through the years sorted by states. Since there's over 50 states we're looking at, it's challenging to discern trends from looking at all the states at once.  
```{r}
breweries.year <- ggplot(beer.df, aes(x = year, y = breweries, group=1))
(p2 <- breweries.year + geom_line() +
   facet_wrap(~STATE, ncol = 10))
```


Let's focus on the state of Florida through the years. We observe an upward trend and then a sudden dip from the late 90's to 2010.  
```{r}
florida.brew.df <- beer.df %>% 
  filter(STATE == "FL")
florida.brew.df

plot(florida.brew.df$year, florida.brew.df$breweries)
```




Since the range of time in the breweries data is from 1984-2017, let's subset the equivalent years from the sightings data.  
```{r}
# clean up sightings data
fl.sightings <- sightings.raw %>% 
  filter(state == 'FL') %>% 
  mutate(year = as.numeric(format(as.Date(date_time, format="%m/%d/%y"),"%Y"))) %>% 
  filter(year >= 1984) %>% 
  filter(year <= 2017) %>% 
  #group_by(year) %>% 
  count(year) %>% 
  rename(sightings_year = n)
```

Let's take a snapshot of sightings per year in Florida.  
```{r}
plot(fl.sightings$year, fl.sightings$sightings_year, type="l")
```



Let's compare the 2 plots at once. Interesting how the 2 plots follow the same shape until the early 2000s and then diverge drastically afterwards. It makes sense to see an overall trend of increased interest in breweries over time.   
```{r}
plot(florida.brew.df$year, florida.brew.df$breweries, type = "l", col="blue")
par(new=TRUE)
plot(fl.sightings$year, fl.sightings$sightings_year, type="l", col="orange")
legend("topleft",legend=c("Breweries", "Sightings"),
       col=c("blue", "orange"), lty=1:2, cex=0.8)
```


## Alien Movies Per Year

```{r}
#pull in alien movies
alien_movies <- read.csv(file = "../../data/raw/alien_movies_per_year.csv", header = TRUE)
sight <- read.csv(file = "../../data/raw/ufo_sightings.csv", header = TRUE)

all_movies <- read.csv(file = "../../data/raw/number_of_movies_per_year.csv", head = TRUE)

all_movies <- all_movies[nrow(all_movies):1,]

ggplot(alien_movies, aes(x =year, y =num_movies )) + geom_point(shape=1) + labs( title = "Number of Alien Movies Per Year\n1902 - 2018", x = "Year", y = "Alien Movies Released") + theme(plot.title = element_text(hjust = 0.5))
```

One big problem right off the bat - a lot of the early years are only zeros. After the 1902 film (a silent film known as "A Trip to the Moon", if you were curious) is followed by a large number of empty slots. 

Luckily, since the UFO reporting society was founded in 1974, we decided to remove all variables prior to this year. Reports before this year appear in the dataset, but as discussed above we determined these reports would not be live records. They would be "popular" events that were recorded after the fact, or incidents reported years after they happened. Let's see if that improves the look of our scatterpplot.

```{r}
alien_movies_new <- filter(alien_movies, year >= 1974)
alien_movies_new <- filter(alien_movies_new, year < 2018)

alien_movies_new$num_movies <- alien_movies_new$num_movies/all_movies$total_movies
ggplot(alien_movies_new, aes(x =year, y =num_movies )) + geom_point(shape=1) + labs( title = "Number of Alien Movies Per Year\n1974 - 2018", x = "Year", y = "Alien Movies Released") + theme(plot.title = element_text(hjust = 0.5))

```

Still a definite trend upwards. That could have to do with the number of movies released increasing in general each year moreso than an increase in the genre alien movies specifically. 

Anyway, we have to filter the sightings dataset to find more specific information about our dataset. Currently we can't compare the number of movies to our number of sightings. Let's filter the dataset down to sightings taking place in Florida since 1974 and see how they compare to the number of movies released each year. 

```{r}
fl_sightings <- filter(sight, tolower(state) == 'fl' )
fl_sightings$date_time <- as.Date(fl_sightings$date_time, "%m/%d/%y")
tmp <- lapply(strsplit(as.character(fl_sightings$date_time), "-"), `[[`, 1)
tmp2 <- sapply(tmp, "[[", 1)
fl_sightings$year <- as.numeric(tmp2)
fl_sightings_75 <- filter(fl_sightings, year >= 1975)
fl_sightings_75 <- filter(fl_sightings, year < 2018)

sightings_per_year_fl <- as.data.frame(table(fl_sightings_75$year))

#sightings_per_year_fl_75$Freq
movies_sightings <- cbind(sightings_per_year_fl$Freq, alien_movies_new$num_movies)
movies_sightings <- as.data.frame(movies_sightings)

colnames(movies_sightings) <- c("Sightings_Per_Year", "Alien_Movies_Per_Year")

ggplot(movies_sightings, aes(x =Alien_Movies_Per_Year, y =Sightings_Per_Year )) + geom_point(shape=1) + labs( title = "Number of Alien Sightings VS Alien Movies eleased\n1974 - 2018", x = "Alien Movies Released Per Year", y = "UFO Sightings Per Year") + theme(plot.title = element_text(hjust = 0.5))
```

There seem to be several significant outliers. It appears one year there were over 600 alien sightings, but almost no alien movies released. 

```{r}
lm.movies <- lm(Sightings_Per_Year~Alien_Movies_Per_Year, data = movies_sightings)
summary(lm.movies)
```

Results here are interesting. Our R-squared isn't incredible, only capturing 19% of the variance in our data. Without other variables in the model, it's difficult to say if the alien movies per year are useful on their own. 

```{r}
residuals <- resid(lm.movies)
plot(residuals)
```

This definitely does not appear to have no pattern! We could easily draw a curve fitting this data. The variance increases the further we go through the function. Because of the pattern in the error, believe there are other variables in the true model that we're currently missing. This isn't terrible news, because we certainly plan on adding more!


## Number of USAF Bases by State


```{r child='base_analysis.Rmd'}
```

## Per Capita GDP

Per Capita GDP for the United States was pulled from [Open Data Networks](https://www.opendatanetwork.com/dataset/data.hawaii.gov/qnar-gix3). Real GDP is an inflation-adjusted measure of each Stateâs gross product that is based on national prices for the goods and services produced within that State. This price is presented in current dollars at the time of the dataset's creation (last updated August 2016). Estimate of the population is based on midyear measurements from the Censue Bureau. 

```{r}
gdp_per_capita <- read.csv(file = "../../data/raw/gdp_per_capita_per_year.csv", header = TRUE)

fl_sightings <- filter(sight, tolower(state) == 'fl' )
fl_sightings$date_time <- as.Date(fl_sightings$date_time, "%m/%d/%y")
tmp <- lapply(strsplit(as.character(fl_sightings$date_time), "-"), `[[`, 1)
tmp2 <- sapply(tmp, "[[", 1)
fl_sightings$year <- as.numeric(tmp2)
fl_sightings_75 <- filter(fl_sightings, year >= 1974)
fl_sightings_75 <- filter(fl_sightings_75, year <= 2014)

sightings_per_year_fl <- as.data.frame(table(fl_sightings_75$year))

gdp_per_capita <- filter(gdp_per_capita, Year.and.category >= 1974)
gdp_sightings <- cbind(sightings_per_year_fl$Freq,gdp_per_capita$Per.capita.GDP..current....)
gdp_sightings <- as.data.frame(gdp_sightings)

colnames(gdp_sightings) <- c("Sightings_Per_Year", "GDP_Per_Capita")

gdp.results <- lm(Sightings_Per_Year ~ GDP_Per_Capita, data = gdp_sightings)

ggplot(gdp_sightings, aes(x =GDP_Per_Capita, y =Sightings_Per_Year )) + geom_point(shape=1) + labs( title = "GDP Per Capita VS UFO Sightings Per Year\n1974 - 2018", x = "GDP Per Capita", y = "UFO Sightings Per Year") + theme(plot.title = element_text(hjust = 0.5))
```

The scatterplot suggests that some kind of polynomial might fit the data best - a cubic term comes to mind. However, it's too soon for us to worry about modeling this term before we get a look at the a full model. 

```{r}
lm.fit <- lm(Sightings_Per_Year ~ GDP_Per_Capita,data = gdp_sightings)
summary(lm.fit)
```
$R^2$ is looking much better than it did with our aliens_movies data, capturing over 50% of the variation. 

## Portion of population with internet access by year

# Data Analysis

## Model Selection

All the data discussed in the EDA section was pulled into one dataset to be imported into R.  
```{r}
all_florida <- read.csv(file = "../../data/raw/all_florida.csv", header = TRUE)
all_florida <- filter(all_florida, year >= 1998)

internet_rurality <- read.csv(file = "../../data/raw/internet_by_rurality.csv", header = TRUE)

all.model <- lm(sightings_year ~ per_capita_gdp_current + breweries + num_movies, data = all_florida)
summary(all.model)
plot(all.model$residuals)
```

Unfortunately, there are several NAs in our database which makes it difficult to use all the variables in our model. Internet by rurality only exists for 2000 to 2015, which is a very small sample size. 

Since we're choosing to only model Florida as a time series, the base distance from an air force base will also not make sense to include in the model. 

```{r}
all_years <- read.csv(file ="../../data/raw/all_states_normalized.csv")
#shave of 2016 for test set
all_years_training <- filter(all_years, year < 2016)
all_years_test <- filter(all_years, year >= 2016)
all.years.lm <- lm(sightings_per_100k ~ region + year + breweries + afbase_per_state + per_capita_gdp_current + alienmovies_per_year,data = all_years_training)

allpossreg <- regsubsets(sightings_per_100k ~ region + year + breweries + afbase_per_state + per_capita_gdp_current + alienmovies_per_year, data = all_years_training, nbest=6)

aprout <- summary(allpossreg)

mallow <- with(aprout,round(cbind(which,cp),3))

ggplot(all_years_training, aes(x =seq(1, length(alienmovies_per_year)), y = alienmovies_per_year )) + geom_point(shape=20, colour = all_years_training$year) + labs( title = "Percentage of Alien Movies Per Year\n1974 - 2015", x = "Index", y = "Percentage of alien movies per year") + theme(plot.title = element_text(hjust = 0.5))

ggplot(all_years_training, aes(x =seq(1, length(alienmovies_per_year)), y = per_capita_gdp_current )) + geom_point(shape=20, colour = all_years_training$year) + labs( title = "GDP Per Capita Per Year\n1974 - 2015", x = "Index", y = "GDP Per Capita") + theme(plot.title = element_text(hjust = 0.5))

ggplot(all_years_training, aes(x = per_capita_gdp_current, y = sightings_per_100k)) + geom_point(shape=20, colour = all_years_training$year) + labs( title = "GDP Per Year Vs Sightings\n1974 - 2015", x = "GDP Per Capita", y = "UFO Sightings") + theme(plot.title = element_text(hjust = 0.5))


```

### R-Squared

$R^2$ is a measure of variability in the dependent variable captured with the variables. As mentioned in the lecture, regardless of any other measures of quality, a low r-squared would indicate that our independent variables were just not appropriate. Luckily, many of our models clear .60 with and two are even over .90. This gives us some level of confidence in our model going forward. However, r-squared does not penalize multiple variables, so we cannot just choose the highest r-squared and believe we have a flexible model to approach other states with. 

### Adjusted R-Squared

Adjusted R-Squared is better at punishing variables included in a model that do not add much to its predictive power. Still, we have two r-squareds with over .90. Both those models, one including number of alien movies per year (featured as num_movies here) and one without, have almost equal adjusted r-squareds. Based only on this information, it would be difficult to choose a model. However, we do not want to stop at $R^2$ regardless because it is too lenient with unnecessary additions to the model. 

### BIC

BIC (Bayesian Information Criterion) does well with judging the best model for the observed data if one of the models up for consideration is the true model. However, the true model for how reports are submitted to the National UFO Reporting Center's website is most likely tossed up with it's ranking in Google Search Results, the number of employees/volunteers the organization has, and other factors not measured by our particular data. We are more focused on creating a working approximate model rather than the "true" model and for this reason with favor AIC.

### Mallow's CP

A small value of CP is associated with a relatively precise model. The smallest CP here is associated with our model that drops the number of alien movies. 

This isn't surprising. The movies only model we created earlier did not seem to have impressive predictability. It is also worth noting it only addresses the raw number of movies coming out, and they are in no way weighted by their popularity in the popular consciousness. Due to these flaws, we do not see a strong theoretical reason these values should be included in the model. 

Because of this, we feel it is acceptable to drop the number of movies from the model and select the model with the lowest value for Mallow's CP.

```{r}
best.fl.model <- lm(sightings_year ~ per_capita_gdp_current + breweries, data = all_florida)

summary(best.fl.model)
plot(best.fl.model$residuals)
```

## Diagnostics

Just because we've use model selection to choose the best model doesn't mean we have a perfect model. 

In fact, for us to put faith in our least squares regression model there are a variety of assumptions we've made. The error terms are supposed to be independent. The error should be normally distributed. The errors should have a constant variance. We don't have a way to know the true error, so we will use the residuals for the model as an approximation of the error. 

If these assumptions are violated, we could have multicollinearity in our models (indicating our independent variables are correlated), we could have heteroskedasticity, or patterns in the residuals suggesting missing independent variables in the models.

### Pairwise Scatterplots 

```{r}
plot(all_florida)
```

There do appear to be some non-linear associations between our variables breweries and per-capita gdp. Since these are the only two variables in our model, this creates some concern there may be some multi-collinearity. Let's see that more closely.

```{r}
plot(all_florida$per_capita_gdp_current, all_florida$breweries)
```

There definitely appears to be a pattern here. With 3 turns, it looks like it may be some cubic predictor. 

Below, we plot the residuals against both our predictors. 
```{r}
plot(all_florida$per_capita_gdp_current,best.fl.model$residuals, main = "Residuals vs Per Capita GDP")

plot(all_florida$breweries, best.fl.model$residuals, main = "Breweries Per Year")
```

```{r}
plot(best.fl.model, which = 1)
```

The residuals vs the fitted values show that we have a few definite outliers at point 6, 13, and 15. 

### Heteroskedasticity
```{r}
ols_bp_test(best.fl.model)
```
The test for Breusch Pagan test fails to reject the null hyptothesis that our variance is constant, suggesting we don't have heteroskedasticity in our model. While the formal mechanical method fails to catch it, our plots suggest there may be non-constant variance in our model.

### Normality

```{r}
plot(best.fl.model, which = 2)

shapiro.test(residuals(best.fl.model))
```

The QQ-plot does not look good. The Shapiro-Wilk test just barely fails to reject the null hypothesis that the data are sampled from a normally distributed population. So the test suggests the data are normal, but what we observe in the Normal Q-Q plot. As we have a small sample, it makes sense that our test might fail to reject the null hypothesis due to random variation expected in the sample anyway. 



## Cross-Validation

(TBD - using the model we've made to predict the values of sightings for other states as part of cross-validation)

# Conclusion

## Results

It is likely that the true value we are measuring is population. A greater number of breweries is most likely capturing the actual value of interest, which is the population of the state. A state with a higher population is more likely to more breweries. This does not necessarily capture the "amount of alchohol consumed per capita" that we were interested in capturing.

Still, understanding we may not have created a model allowing for statistical inference, we still have some predictive power. 

## Future Work

### Implementing Advanced Spatial-Temporal & Time-Series Models
With the limited scope of our understanding of statistics, we are unable to compare all five explanatory factors with the predicted variable, sightings per year. As we learn more advanced statistical techniques, we can come back to this analysis and improve on modelling with spatial and time dependencies.  

### Greater Sample Size

The number of sightings was healthy, over 80,000 reports filed. However, the other data was highly constricted in the amount of years it contained, some having as few as 15 observations. This is insufficient to create the kind of robust model that would be effective for all 50 states.

### Investigate Other States

Here, we identified 5 states that were "super states" in their number of sightings as identified as the density of sightings per state. It may be the case that the number of sightings in these super states and lesser states are fundamentally different, requiring a different model. Or, the "super state" identification may actually be a variable that needs to be considered with a dummy variable to change the intercept for as oppossed to "regular" states.

### Investigate Most Popular Alien Movies  
Instead of using alien movies per year, we might use the amount of money grossed in aliens films per year. This would give a greater indicator of a film's affect on the popular consciousness. A movie like "Aliens" surely had a greater effect than "Mars Needs Moms", for example. Our current scraping of the data, relying on each movie as a equally effective unit and simply counting the number of movies per year is likely missing a lot of that information. 

# Works Cited/ Data Sources
 *  Open Data Network. 2012. [Data and codebook] (https://www.opendatanetwork.com/dataset/data.hawaii.gov/qnar-gix3)  
 
 * National UFO Reporting Center. 2017.[Website data] (http://www.nuforc.org/webreports.html)  
 
 * Wikipedia. 2017 [Website data] (https://en.wikipedia.org/wiki/List_of_films_featuring_extraterrestrials)
 
 
 
